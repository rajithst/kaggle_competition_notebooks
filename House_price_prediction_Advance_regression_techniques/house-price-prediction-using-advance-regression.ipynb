{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Ongoing kernal","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Basic Data analysis\n1.  Find/Manage missing values\n    * Plot missingness\n    * Drop /Impute missing values\n2. Explore Data by category wise\n    * Quantitative Discrete data\n3. Explore correlations\n    * Correlations with SalePrice with Quantitative continues features\n    * Correlations with SalePrice with Quantitative continues features\n    * Correlations with SalePrice with Quantitative continues features\n    \n4. Identify/Remove Outliers \n5. Explore dependent variable\n\n### Regression Analysis\n6. Implement Baseline Regression\n7. Advance Regression methods","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno as msno\n\n#sklearn imports\nfrom sklearn.preprocessing import OrdinalEncoder\n#imputer\nfrom fancyimpute import KNN\nfrom fancyimpute import IterativeImputer\n\n#visulizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#statistics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nlocal=False\nif local is False:\n    import os\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"if local == False:\n    traindf=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n    testdf=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nelse:\n    traindf=pd.read_csv(\"train.csv\")\n    testdf=pd.read_csv(\"test.csv\")\nprint(traindf.shape)\nprint(testdf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **1. Find/Manage missing values**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"combine train and test data to explore missing values and further analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_ids = traindf[\"Id\"]\ntesting_ids = testdf[\"Id\"]\ntraindf_cp = traindf.copy()\ntestdf_cp = testdf.copy()\ndependent_data = traindf[[\"Id\",\"SalePrice\"]]\ntraindf_cp.drop(\"SalePrice\",axis=1,inplace=True)\nprint(traindf_cp.shape)\nprint(testdf_cp.shape)\nif traindf_cp.shape[1]==testdf_cp.shape[1]:\n    house_data = pd.concat([traindf_cp,testdf_cp],axis=0)\n    house_data = house_data.reset_index(drop=True)\n    house_data.fillna(np.nan,inplace=True)\n    print(house_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets explore !!!!!!!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"print information of the dataset,so we can get idea about default data types and missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"custom funtion to show the percentage of missing rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_missing_info(house_data):\n    missing_info = house_data.isna().sum().reset_index(drop=False)\n    missing_info.columns = [\"column\",\"rows\"]\n    missing_info[\"missing_pct\"] = (missing_info[\"rows\"]/house_data.shape[0])*100\n    missing_info = missing_info[missing_info[\"rows\"]>0].sort_values(by=\"missing_pct\",ascending=False)\n    return missing_info\nmissing_df = show_missing_info(house_data)\nmissing_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 1. ####  Plot the missingness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(house_data,labels=house_data.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(house_data,labels=house_data.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Imputing/Drop missing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Select feaures that missing rows are less than 20 and delete those rows\n2. Drop PoolQC,MiscFeature,Alley features from the dataset because of over 90% of the data is missing and imputing will add bias to the model\n\n3. keep Fence(Fence quality) feature and Impute missing rows since it is considerable effect to target variable ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"delete_rows_cols = missing_df[missing_df[\"rows\"]<20][\"column\"].tolist()\nhouse_data.dropna(axis=0,how=\"any\",subset=delete_rows_cols,inplace=True)\nprint(house_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data.drop(columns=[\"PoolQC\",\"MiscFeature\",\"Alley\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data_columns = house_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Impute Categorical features using KNN Imputer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = \"Fence,FireplaceQu,GarageFinish,GarageQual,GarageCond,GarageType,BsmtExposure,BsmtCond,BsmtQual,BsmtFinType2,BsmtFinType1,MasVnrType\".split(\",\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"use ordinal_encoding function to encode categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ordinal_encoding(data):\n    #empty dictionary ordinal_enc_dict\n    ordinal_enc_dict = {}\n    for col_name in categorical_columns:\n        # Create Ordinal encoder for col\n        ordinal_enc_dict[col_name] = OrdinalEncoder()\n        col = data[col_name]\n\n        # Select non-null values of col\n        col_not_null = col[col.notnull()]\n        reshaped_vals = col_not_null.values.reshape(-1, 1)\n        encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n\n        # Store the values to non-null values of the column in users\n        data.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)\n    return data,ordinal_enc_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data_encoded,encoded_dict = ordinal_encoding(house_data.copy(deep=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data_encoded[categorical_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_columns:\n    print(house_data_encoded[i].unique())\n    print(\"-\"*40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Impute categorical features using KNN algorithm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_categorical_features(data,encoded_dict):\n    # Create KNN imputer\n    KNN_imputer = KNN()\n    data.iloc[:, :] = np.round(KNN_imputer.fit_transform(data))\n    for col_name in categorical_columns:\n        reshaped = data[col_name].values.reshape(-1, 1)\n        data[col_name] = encoded_dict[col_name].inverse_transform(reshaped)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data_imputed = impute_categorical_features(house_data_encoded[categorical_columns],encoded_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data.drop(columns=categorical_columns,inplace=True,axis=1)\nhouse_data = pd.concat([house_data,house_data_imputed],axis=1)\nhouse_data = house_data[house_data_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data[categorical_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = show_missing_info(house_data)\nmissing_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Impute Continues missing values using Multiple Imputation by Chained Equations (MICE)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_columns = missing_df[\"column\"].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmpdf = house_data[missing_columns]\ntmpdf_index = tmpdf.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MICE_imputer = IterativeImputer()\nhouse_data_mice = MICE_imputer.fit_transform(tmpdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_df = pd.DataFrame(house_data_mice,columns=missing_columns,index=tmpdf_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data.drop(columns=missing_columns,inplace=True,axis=1)\nhouse_data = pd.concat([house_data,imputed_df],axis=1)\nprint(house_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = show_missing_info(house_data)\nmissing_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we have imputed all missing data..it's time to deep dive in to further analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Split Merged dataset to original training and Testing datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pp_house_data_train = house_data[house_data[\"Id\"].isin(training_ids.tolist())]\npp_house_data_test = house_data[house_data[\"Id\"].isin(testing_ids.tolist())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_price = dependent_data[dependent_data[\"Id\"].isin(training_ids.tolist())]\npp_house_data_train.insert(0,\"SalePrice\",sales_price[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp_house_data_train.to_csv(\"pp_train.csv\",index=False)\npp_house_data_test.to_csv(\"pp_test.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### Explore data by Data type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"pp_train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's split our dataset to subsets of each data type.This will easy to explore,plot and identify correlations.We will reduce dimentions of the dataset later after we implement baseline model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"area_features = \"LotFrontage,LotArea,MasVnrArea,BsmtFinSF1,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,1stFlrSF,2ndFlrSF,LowQualFinSF,GrLivArea,GarageArea,WoodDeckSF,OpenPorchSF,EnclosedPorch,3SsnPorch,ScreenPorch,PoolArea\".split(\",\")\nyears_and_dates = \"YearBuilt,YearRemodAdd,GarageYrBlt,MoSold,YrSold\".split(\",\")\nquantitiative_descrete_columns = \"FullBath,HalfBath,BsmtFullBath,BsmtHalfBath,TotRmsAbvGrd,Fireplaces,GarageCars,MiscVal\".split(\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = list(set(list(train_df.columns[2:])) - set(area_features+years_and_dates+quantitiative_descrete_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in area_features+quantitiative_descrete_columns+years_and_dates:\n    train_df[i] = train_df[i].astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overview of date type data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"summary of years and dates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[years_and_dates].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the summary of the dates and years data,we can see there are no abnormal data points.All features are in valid range.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Overview of Quantitative Discrete data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[quantitiative_descrete_columns].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets explore correlations\n* Check for highly correlated features.Highly correlated features are does not add extra information to the dataset.so we can remove those features and reduce the dimentionality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train_df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nfig, ax = plt.subplots(figsize=(20,15))\nsns.heatmap(corr,mask=mask,center=0, linewidths=0, annot=True, fmt=\".2f\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see,we don't have any highly correlated pair (correlation >90)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlation Analysis with SalePrice","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"House area and other area features are always has huge impact on cost of the property.So we explore area features separately","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"qc_correlation_with_saleprice = train_df[[\"SalePrice\"]+area_features].corr()[\"SalePrice\"][1:]\nfig = plt.subplots(figsize=(16, 5))\nplt.bar(qc_correlation_with_saleprice.index,qc_correlation_with_saleprice)\nplt.xticks(rotation=90)\nplt.title(\"quantitative continues features correlation with sales price\")\nplt.xlabel(\"category\")\nplt.ylabel(\"correlation\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GrLivArea**,**GarageArea**,**1stFkrSF**,**TotalBsmtSF** are having significant positive correlation (>0.6) with SalePrice.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Correlations of  SalePrice  with Quantitative Descrete features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"qd_correlation_with_saleprice = train_df[[\"SalePrice\"]+quantitiative_descrete_columns].corr()[\"SalePrice\"][1:]\nfig = plt.subplots(figsize=(16, 5))\nplt.bar(qd_correlation_with_saleprice.index,qd_correlation_with_saleprice)\nplt.xticks(rotation=90)\nplt.title(\"quantitiative descrete features correlation with sales price\")\nplt.xlabel(\"category\")\nplt.ylabel(\"correlation\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FullBath**,**TotRmsAbvGrd**,**GarageCars** are having significant correlation with SalePrice","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Correlations of  SalePrice  with Categorical Nominal features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cn_correlation_with_saleprice = train_df[[\"SalePrice\"]+categorical_features].corr()[\"SalePrice\"][1:]\nfig = plt.subplots(figsize=(16, 5))\nplt.bar(cn_correlation_with_saleprice.index,cn_correlation_with_saleprice)\nplt.xticks(rotation=90)\nplt.title(\"Categorical features correlation with sales price\")\nplt.xlabel(\"category\")\nplt.ylabel(\"correlation\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No surprise,overall Quality is a significant feature for price of the property. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Explore data and Identify Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets take these important features and check if there are possible outliers,before that lets plot and check how features behaves with SalePrice.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(6,3,figsize=(15,15))\nsplits = np.split(np.array(area_features), 6)\nfor k,sp in enumerate(splits):\n    for i,col in enumerate(sp):\n        ax[k,i].scatter(train_df[col],train_df[\"SalePrice\"])\n        ax[k,i].set_title(col)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(4,2,figsize=(15,15))\nsplits = np.split(np.array(quantitiative_descrete_columns), 4)\nfor k,sp in enumerate(splits):\n    for i,col in enumerate(sp):\n        ax[k,i].scatter(train_df[col],train_df[\"SalePrice\"])\n        ax[k,i].set_title(col)\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find high correlated features and plot to identify possible outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_selected_columns_scatter(selected_columns):\n    for i in selected_columns:\n        fig,ax = plt.subplots(figsize=(15,4))\n        plt.scatter(train_df[i],train_df[\"SalePrice\"])\n        plt.title(f\"{i} vs Sale Price\")\n        plt.xlabel(i)\n        plt.ylabel(\"Sale Price\")\n        plt.grid()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"selected_columns_qc = qc_correlation_with_saleprice[qc_correlation_with_saleprice>0.5].sort_values(ascending=False).index\nselected_columns_qd = qd_correlation_with_saleprice[qd_correlation_with_saleprice>0.5].sort_values(ascending=False).index\nplot_selected_columns_scatter(selected_columns_qc.tolist()+selected_columns_qd.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"We can see there are possible outliers of these important features,this will hurt the model.lets remove outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Remove outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"simple function to define range and keep only data within that range","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = {\"GrLivArea\":{\"sales_price\":200000,\"value\":4000},\n            \"GarageArea\":{\"sales_price\":300000,\"value\":1200},\n            \"TotalBsmtSF\":{\"sales_price\":200000,\"value\":6000},\n            \"1stFlrSF\":{\"sales_price\":200000,\"value\":4000}}\nprint(train_df.shape)\nfor col in selected_columns_qc:\n    ol = outliers.get(col)\n    train_df = train_df[~((train_df[\"SalePrice\"] < ol[\"sales_price\"]) & (train_df[col] > ol[\"value\"]))]\nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_selected_columns_scatter(selected_columns_qc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore target variable","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"lets see how our target variable is looks like.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,5))\nsns.distplot(train_df[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"our target variable follows right skewed distribution.Lets transform this to normal distribution using log transformation.Since our target variable and predictions are large numbers getting, root_mean_squared_error of log transformed values will not punish for larger errors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_sale_prices = traindf[\"SalePrice\"]\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\nfig,ax = plt.subplots(figsize=(10,5))\nsns.distplot(train_df[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### prepare data for machine learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(\"Id\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.get_dummies(train_df,columns=categorical_features,drop_first=True)\ntrain_df = train_df.apply(pd.to_numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### baseline model using all features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(data):\n    X = data.drop(\"SalePrice\",axis=1)\n    y = data[\"SalePrice\"]\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n    return X_train,X_test,y_train,y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_baseline_model(X_train,X_test,y_train,y_test):\n    model = LinearRegression()\n    model.fit(X_train,y_train)\n    print(f\"Training set R2 {model.score(X_train,y_train)}\")\n    y_pred = model.predict(X_test)\n    print(f\"R2 score {r2_score(y_test,y_pred)}\")\n    print(f\"RMSE {np.sqrt(mean_squared_error(y_test,y_pred))}\")\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_pred_and_test(y_test,y_pred):\n    plt.figure(figsize=(20,5))\n    plt.plot(y_test.values,label=\"Actual\")\n    plt.plot(y_pred,label=\"Predicted\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = prepare_data(train_df)\ny_pred = fit_baseline_model(X_train,X_test,y_train,y_test)\nshow_pred_and_test(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":":\n### Goal is to perform better than baseline model,which is rmse less than 0.117","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets try to improve current linear regression using knowledge of ensemble learning.\nLets sample data without replacement","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = prepare_data(train_df)\nbag_reg = BaggingRegressor(LinearRegression(),\n                          n_estimators=200,\n                          bootstrap=True,\n                          max_samples=0.7,\n                          n_jobs=-1,\n                          oob_score=True)\nbag_reg.fit(X_train,y_train)\nprint(f\"Out of bag score {bag_reg.oob_score_}\")\nprint(f\"Training set R2 {bag_reg.score(X_train,y_train)}\")\ny_pred = bag_reg.predict(X_test)\nprint(f\"R2 score {r2_score(y_test,y_pred)}\")\nprint(f\"RMSE {np.sqrt(mean_squared_error(y_test,y_pred))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_pred_and_test(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We just added bagging using linear regression and our model has improved.Lets deep dive in to ensemble using some advance ensemble techniques.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nX_train,X_test,y_train,y_test = prepare_data(train_df)\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Run individual regressors with cross validation to find better individual predictors with better hyperparams\n* Stack predictors and using cross validation\n* Bleding models\n* submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nelastic = DecisionTreeRegressor(max_depth=20).fit(X_train,y_train)\nprint(f\"Training set R2 {elastic.score(X_train,y_train)}\")\ny_pred = elastic.predict(X_test)\nprint(f\"R2 score {r2_score(y_test,y_pred)}\")\nprint(f\"RMSE {np.sqrt(mean_squared_error(y_test,y_pred))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_reg = xgb.XGBRegressor(learning_rate=0.01,n_estimators=3500,\n                                     max_depth=5, min_child_weight=5,\n                                     gamma=0.0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear').fit(X_train,y_train)\nprint(f\"Training set R2 {xgb_reg.score(X_train,y_train)}\")\ny_pred = xgb_reg.predict(X_test)\nprint(f\"R2 score {r2_score(y_test,y_pred)}\")\nprint(f\"RMSE {np.sqrt(mean_squared_error(y_test,y_pred))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_reg= AdaBoostRegressor(RandomForestRegressor(),\n                            n_estimators=100,\n                            learning_rate=1.0)\nada_reg.fit(X_train,y_train)\nprint(f\"Training set R2 {ada_reg.score(X_train,y_train)}\")\ny_pred = ada_reg.predict(X_test)\nprint(f\"R2 score {r2_score(y_test,y_pred)}\")\nprint(f\"RMSE {np.sqrt(mean_squared_error(y_test,y_pred))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AdaBoostRegressor(base_estimator=DecisionTreeRegressor(),\n                            n_estimators=100,\n                            learning_rate=1.0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}